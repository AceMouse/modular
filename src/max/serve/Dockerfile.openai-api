# This dockerfile is meant to be published externally on Dockerhub, and demonstrates the max serve pipelines.

# We define CUDA_VERSION twice because an ARG's scope lasts till FROM, see few lines below
# The CUDA version should match what we use in CI, MDCM and locally
# see https://github.com/modularml/modular/blob/main/utils/docker/Dockerfile#L201-L220
# https://github.com/modularml/modular/blob/main/utils/install-dependencies.py#L265-L313
# https://github.com/modularml/infra/blob/main/tools/image_builder/packer/aws/global.pkr.hcl#L48-L51
ARG CUDA_VERSION=12.5.0

# use a "build" image to create the conda env & copy the resulting env into the "real image
# this lets us separate "build" deps from "runtime" deps
# it also excuses us from cleanup since we only copy specific files/dirs from the build image
FROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS build

ARG TARGETPLATFORM

WORKDIR /app

# g++ currently only necessary for the faster-fifo build. See SDLC-1381
RUN apt-get update && apt-get install -y wget g++

# Install Conda
ENV CONDA_DIR=/opt/conda
RUN if [ "$TARGETPLATFORM" = "linux/amd64" ]; then \
    MINICONDA_URL="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"; \
    elif [ "$TARGETPLATFORM" = "linux/arm64" ]; then \
    MINICONDA_URL="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh"; \
    else \
    echo "Unsupported architecture: $TARGETPLATFORM"; \
    exit 1; \
    fi && \
    wget --quiet $MINICONDA_URL -O ~/miniconda.sh && \
    /bin/bash ~/miniconda.sh -b -p $CONDA_DIR
ENV PATH=$CONDA_DIR/bin:$PATH

ENV ENV_DIR=/opt/venv

# Create a conda env
COPY SDK/lib/API/python/max/serve/env.yml .
RUN conda env create -f env.yml -p $ENV_DIR --verbose

# Copy and install MAX conda packages
RUN mkdir -p conda/
COPY tmp/conda/ conda/
RUN MAX_CONDA_CORE_FILENAME=$(find -type f -name 'max-core-*-release.conda') && conda install -p $ENV_DIR -y $MAX_CONDA_CORE_FILENAME && \
    MAX_CONDA_FILENAME=$(find -type f -name 'max-python-*-3.12release.conda') && conda install -p $ENV_DIR -y $MAX_CONDA_FILENAME


FROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04

ARG CUDA_VERSION=12.5.0

WORKDIR /app

# Copy the env into this image
ENV ENV_DIR=/opt/venv
COPY --from=build $ENV_DIR $ENV_DIR

COPY Licenses/MAX-Platform-Software-License ./MAX-Platform-Software-License.txt
COPY Licenses/Third-Party-Notices ./Third-Party-Notices.txt
COPY SDK/lib/API/python/max/serve/README.PUBLIC.md README.md
# Copy Pipelines code
COPY SDK/public/max-repo/pipelines/python/ .
# Copy Entrypoint
COPY SDK/lib/API/python/max/serve/entrypoint.sh .

# Symlink CUDA so that the Engine can find it
RUN mkdir -p /usr/lib64/nvidia && ln -s /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/libcuda.so /usr/lib64/nvidia/libcuda.so

# This env gets set after max gets installed
# but not visible for sh so we set it explicitly here
ENV PATH=$ENV_DIR/bin:$PATH
ENV MODULAR_HOME=$ENV_DIR/share/max
# suppress transformers logging
ENV TRANSFORMERS_VERBOSITY="critical"
ENV TRANSFORMERS_NO_ADVISORY_WARNINGS=1
ENV MODULAR_STRUCTURED_LOGGING=1
ENV PROMETHEUS_MULTIPROC_DIR=/tmp

EXPOSE 8000

ENTRYPOINT ["./entrypoint.sh"]
