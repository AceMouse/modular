# ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
# REQUIRES: H100-GPU
# RUN: %mojo-no-debug-no-assert %s | FileCheck %s

from gpu.host import DeviceContext
from gpu.host._compile import _get_gpu_target
from gpu.memory import AddressSpace
from gpu.id import thread_idx
from gpu.sync import barrier


from layout._utils import ManagedLayoutTensor
from layout import Layout, LayoutTensor, IntTuple
from layout.fillers import arange
from layout.tensor_core_async import TensorCoreAsync
from layout.layout_tensor import copy_dram_to_sram

from utils import Index


fn tensor_core_async_tf32_tf32_kernel[
    dst_layout: Layout,
    lhs_layout: Layout,
    rhs_layout: Layout,
](
    dst: LayoutTensor[DType.float32, dst_layout],
    lhs: LayoutTensor[DType.float32, lhs_layout],
    rhs: LayoutTensor[DType.float32, rhs_layout],
):
    tensor_core_async = TensorCoreAsync[
        DType.float32, DType.float32, Index(64, 8, 8)
    ]()

    smem_operand_a = tensor_core_async.allocate_lhs()
    smem_operand_b = tensor_core_async.allocate_rhs()

    if thread_idx.x == 0:
        smem_operand_a.copy_from(lhs)
        smem_operand_b.copy_from(rhs)

    barrier()

    var c_reg = tensor_core_async.allocate_result(0)

    c_reg = tensor_core_async(smem_operand_a, smem_operand_b, c_reg)
    tensor_core_async.commit_group()
    tensor_core_async.wait_for_all()

    tensor_core_async.store_result(dst, c_reg)


def test_tensor_core_async_tf32_tf32_64x8x8(ctx: DeviceContext):
    print("== test_tensor_core_async_tf32_tf32_64x8x8")

    alias M = 64
    alias N = 8
    alias K = 8

    lhs = ManagedLayoutTensor[DType.float32, Layout.row_major(M, K)](ctx)
    arange(lhs.tensor())

    rhs = ManagedLayoutTensor[DType.float32, Layout.row_major(K, N)](ctx)
    arange(rhs.tensor())

    dst = ManagedLayoutTensor[DType.float32, Layout.row_major(M, N)](ctx)

    func = ctx.compile_function[
        tensor_core_async_tf32_tf32_kernel[dst.layout, lhs.layout, rhs.layout],
        _target = _get_gpu_target["sm_90"](),
    ]()

    ctx.enqueue_function(
        func,
        dst.device_tensor(),
        lhs.device_tensor(),
        rhs.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()
    # CHECK-LABEL: test_tensor_core_async_tf32_tf32_64x8x8
    # CHECK: 1120.0 1148.0 1176.0 1204.0 1232.0 1260.0 1288.0 1316.0
    # CHECK: 2912.0 3004.0 3096.0 3188.0 3280.0 3372.0 3464.0 3556.0
    # CHECK: 4704.0 4860.0 5016.0 5172.0 5328.0 5484.0 5640.0 5796.0
    # CHECK: 6496.0 6716.0 6936.0 7156.0 7376.0 7596.0 7816.0 8036.0
    # CHECK: 8288.0 8572.0 8856.0 9140.0 9424.0 9708.0 9992.0 10276.0
    # CHECK: 10080.0 10428.0 10776.0 11124.0 11472.0 11820.0 12168.0 12516.0
    # CHECK: 11872.0 12284.0 12696.0 13108.0 13520.0 13932.0 14344.0 14756.0
    # CHECK: 13664.0 14140.0 14616.0 15092.0 15568.0 16044.0 16520.0 16996.0
    # CHECK: 15456.0 15996.0 16536.0 17076.0 17616.0 18156.0 18696.0 19236.0
    # CHECK: 17248.0 17852.0 18456.0 19060.0 19664.0 20268.0 20872.0 21476.0
    # CHECK: 19040.0 19708.0 20376.0 21044.0 21712.0 22380.0 23048.0 23716.0
    # CHECK: 20832.0 21564.0 22296.0 23028.0 23760.0 24492.0 25224.0 25956.0
    # CHECK: 22624.0 23420.0 24216.0 25012.0 25808.0 26604.0 27400.0 28196.0
    # CHECK: 24416.0 25276.0 26136.0 26996.0 27856.0 28716.0 29576.0 30436.0
    # CHECK: 26208.0 27132.0 28056.0 28980.0 29904.0 30828.0 31752.0 32676.0
    # CHECK: 28000.0 28988.0 29976.0 30964.0 31952.0 32940.0 33928.0 34916.0
    # CHECK: 29792.0 30844.0 31896.0 32948.0 34000.0 35052.0 36104.0 37156.0
    # CHECK: 31584.0 32700.0 33816.0 34932.0 36048.0 37164.0 38280.0 39396.0
    # CHECK: 33376.0 34556.0 35736.0 36916.0 38096.0 39276.0 40456.0 41636.0
    # CHECK: 35168.0 36412.0 37656.0 38900.0 40144.0 41388.0 42632.0 43876.0
    # CHECK: 36960.0 38268.0 39576.0 40884.0 42192.0 43500.0 44808.0 46116.0
    # CHECK: 38752.0 40124.0 41496.0 42868.0 44240.0 45612.0 46984.0 48356.0
    # CHECK: 40544.0 41980.0 43416.0 44852.0 46288.0 47724.0 49160.0 50596.0
    # CHECK: 42336.0 43836.0 45336.0 46836.0 48336.0 49836.0 51336.0 52836.0
    # CHECK: 44128.0 45692.0 47256.0 48820.0 50384.0 51948.0 53512.0 55076.0
    # CHECK: 45920.0 47548.0 49176.0 50804.0 52432.0 54060.0 55688.0 57316.0
    # CHECK: 47712.0 49404.0 51096.0 52788.0 54480.0 56172.0 57864.0 59556.0
    # CHECK: 49504.0 51260.0 53016.0 54772.0 56528.0 58284.0 60040.0 61796.0
    # CHECK: 51296.0 53116.0 54936.0 56756.0 58576.0 60396.0 62216.0 64036.0
    # CHECK: 53088.0 54972.0 56856.0 58740.0 60624.0 62508.0 64392.0 66276.0
    # CHECK: 54880.0 56828.0 58776.0 60724.0 62672.0 64620.0 66568.0 68516.0
    # CHECK: 56672.0 58684.0 60696.0 62708.0 64720.0 66732.0 68744.0 70756.0
    # CHECK: 58464.0 60540.0 62616.0 64692.0 66768.0 68844.0 70920.0 72996.0
    # CHECK: 60256.0 62396.0 64536.0 66676.0 68816.0 70956.0 73096.0 75236.0
    # CHECK: 62048.0 64252.0 66456.0 68660.0 70864.0 73068.0 75272.0 77476.0
    # CHECK: 63840.0 66108.0 68376.0 70644.0 72912.0 75180.0 77448.0 79716.0
    # CHECK: 65632.0 67964.0 70296.0 72628.0 74960.0 77292.0 79624.0 81956.0
    # CHECK: 67424.0 69820.0 72216.0 74612.0 77008.0 79404.0 81800.0 84196.0
    # CHECK: 69216.0 71676.0 74136.0 76596.0 79056.0 81516.0 83976.0 86436.0
    # CHECK: 71008.0 73532.0 76056.0 78580.0 81104.0 83628.0 86152.0 88676.0
    # CHECK: 72800.0 75388.0 77976.0 80564.0 83152.0 85740.0 88328.0 90916.0
    # CHECK: 74592.0 77244.0 79896.0 82548.0 85200.0 87852.0 90504.0 93156.0
    # CHECK: 76384.0 79100.0 81816.0 84532.0 87248.0 89964.0 92680.0 95396.0
    # CHECK: 78176.0 80956.0 83736.0 86516.0 89296.0 92076.0 94856.0 97636.0
    # CHECK: 79968.0 82812.0 85656.0 88500.0 91344.0 94188.0 97032.0 99876.0
    # CHECK: 81760.0 84668.0 87576.0 90484.0 93392.0 96300.0 99208.0 102116.0
    # CHECK: 83552.0 86524.0 89496.0 92468.0 95440.0 98412.0 101384.0 104356.0
    # CHECK: 85344.0 88380.0 91416.0 94452.0 97488.0 100524.0 103560.0 106596.0
    # CHECK: 87136.0 90236.0 93336.0 96436.0 99536.0 102636.0 105736.0 108836.0
    # CHECK: 88928.0 92092.0 95256.0 98420.0 101584.0 104748.0 107912.0 111076.0
    # CHECK: 90720.0 93948.0 97176.0 100404.0 103632.0 106860.0 110088.0 113316.0
    # CHECK: 92512.0 95804.0 99096.0 102388.0 105680.0 108972.0 112264.0 115556.0
    # CHECK: 94304.0 97660.0 101016.0 104372.0 107728.0 111084.0 114440.0 117796.0
    # CHECK: 96096.0 99516.0 102936.0 106356.0 109776.0 113196.0 116616.0 120036.0
    # CHECK: 97888.0 101372.0 104856.0 108340.0 111824.0 115308.0 118792.0 122276.0
    # CHECK: 99680.0 103228.0 106776.0 110324.0 113872.0 117420.0 120968.0 124516.0
    # CHECK: 101472.0 105084.0 108696.0 112308.0 115920.0 119532.0 123144.0 126756.0
    # CHECK: 103264.0 106940.0 110616.0 114292.0 117968.0 121644.0 125320.0 128996.0
    # CHECK: 105056.0 108796.0 112536.0 116276.0 120016.0 123756.0 127496.0 131236.0
    # CHECK: 106848.0 110652.0 114456.0 118260.0 122064.0 125868.0 129672.0 133476.0
    # CHECK: 108640.0 112508.0 116376.0 120244.0 124112.0 127980.0 131848.0 135716.0
    # CHECK: 110432.0 114364.0 118296.0 122228.0 126160.0 130092.0 134024.0 137956.0
    # CHECK: 112224.0 116220.0 120216.0 124212.0 128208.0 132204.0 136200.0 140196.0
    # CHECK: 114016.0 118076.0 122136.0 126196.0 130256.0 134316.0 138376.0 142436.0

    print(dst.tensor())

    _ = lhs^
    _ = rhs^
    _ = dst^
    _ = func^


fn tensor_core_async_bf16_bf16_f32_kernel[
    dst_layout: Layout,
    lhs_layout: Layout,
    rhs_layout: Layout,
](
    dst: LayoutTensor[DType.float32, dst_layout],
    lhs: LayoutTensor[DType.bfloat16, lhs_layout],
    rhs: LayoutTensor[DType.bfloat16, rhs_layout],
):
    tensor_core_async = TensorCoreAsync[
        DType.float32, DType.bfloat16, Index(64, 8, 16)
    ]()

    smem_operand_a = tensor_core_async.allocate_lhs()
    smem_operand_b = tensor_core_async.allocate_rhs()

    if thread_idx.x == 0:
        smem_operand_a.copy_from(lhs)
        smem_operand_b.copy_from(rhs)

    barrier()

    var c_reg = tensor_core_async.allocate_result(0)

    c_reg = tensor_core_async(smem_operand_a, smem_operand_b, c_reg)
    tensor_core_async.commit_group()
    tensor_core_async.wait_for_all()

    tensor_core_async.store_result(dst, c_reg)


def test_tensor_core_async_bf16_bf16_f32_64x8x16(ctx: DeviceContext):
    print("== test_tensor_core_async_bf16_bf16_f32_64x8x16")

    alias M = 64
    alias N = 8
    alias K = 16

    lhs = ManagedLayoutTensor[DType.bfloat16, Layout.row_major(M, K)](ctx)
    arange(lhs.tensor())

    rhs = ManagedLayoutTensor[DType.bfloat16, Layout.row_major(K, N)](ctx)
    arange(rhs.tensor())

    dst = ManagedLayoutTensor[DType.float32, Layout.row_major(M, N)](ctx)

    func = ctx.compile_function[
        tensor_core_async_bf16_bf16_f32_kernel[
            dst.layout, lhs.layout, rhs.layout
        ],
        _target = _get_gpu_target["sm_90"](),
    ]()

    ctx.enqueue_function(
        func,
        dst.device_tensor(),
        lhs.device_tensor(),
        rhs.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()
    # CHECK-LABEL: == test_tensor_core_async_bf16_bf16_f32_64x8x16
    # CHECK: 9920.0 10040.0 10160.0 10280.0 10400.0 10520.0 10640.0 10760.0
    # CHECK: 25280.0 25656.0 26032.0 26408.0 26784.0 27160.0 27536.0 27912.0
    # CHECK: 40640.0 41272.0 41904.0 42536.0 43168.0 43800.0 44432.0 45064.0
    # CHECK: 56000.0 56888.0 57776.0 58664.0 59552.0 60440.0 61328.0 62216.0
    # CHECK: 71360.0 72504.0 73648.0 74792.0 75936.0 77080.0 78224.0 79368.0
    # CHECK: 86720.0 88120.0 89520.0 90920.0 92320.0 93720.0 95120.0 96520.0
    # CHECK: 102080.0 103736.0 105392.0 107048.0 108704.0 110360.0 112016.0 113672.0
    # CHECK: 117440.0 119352.0 121264.0 123176.0 125088.0 127000.0 128912.0 130824.0
    # CHECK: 132800.0 134968.0 137136.0 139304.0 141472.0 143640.0 145808.0 147976.0
    # CHECK: 148160.0 150584.0 153008.0 155432.0 157856.0 160280.0 162704.0 165128.0
    # CHECK: 163520.0 166200.0 168880.0 171560.0 174240.0 176920.0 179600.0 182280.0
    # CHECK: 178880.0 181816.0 184752.0 187688.0 190624.0 193560.0 196496.0 199432.0
    # CHECK: 194240.0 197432.0 200624.0 203816.0 207008.0 210200.0 213392.0 216584.0
    # CHECK: 209600.0 213048.0 216496.0 219944.0 223392.0 226840.0 230288.0 233736.0
    # CHECK: 224960.0 228664.0 232368.0 236072.0 239776.0 243480.0 247184.0 250888.0
    # CHECK: 240320.0 244280.0 248240.0 252200.0 256160.0 260120.0 264080.0 268040.0
    # CHECK: 255744.0 259960.0 264176.0 268392.0 272608.0 276824.0 281040.0 285256.0
    # CHECK: 271104.0 275576.0 280048.0 284520.0 288992.0 293464.0 297936.0 302408.0
    # CHECK: 286464.0 291192.0 295920.0 300648.0 305376.0 310104.0 314832.0 319560.0
    # CHECK: 301824.0 306808.0 311792.0 316776.0 321760.0 326744.0 331728.0 336712.0
    # CHECK: 317184.0 322424.0 327664.0 332904.0 338144.0 343384.0 348624.0 353864.0
    # CHECK: 332544.0 338040.0 343536.0 349032.0 354528.0 360024.0 365520.0 371016.0
    # CHECK: 347904.0 353656.0 359408.0 365160.0 370912.0 376664.0 382416.0 388168.0
    # CHECK: 363264.0 369272.0 375280.0 381288.0 387296.0 393304.0 399312.0 405320.0
    # CHECK: 378624.0 384888.0 391152.0 397416.0 403680.0 409944.0 416208.0 422472.0
    # CHECK: 393984.0 400504.0 407024.0 413544.0 420064.0 426584.0 433104.0 439624.0
    # CHECK: 409344.0 416120.0 422896.0 429672.0 436448.0 443224.0 450000.0 456776.0
    # CHECK: 424704.0 431736.0 438768.0 445800.0 452832.0 459864.0 466896.0 473928.0
    # CHECK: 440064.0 447352.0 454640.0 461928.0 469216.0 476504.0 483792.0 491080.0
    # CHECK: 455424.0 462968.0 470512.0 478056.0 485600.0 493144.0 500688.0 508232.0
    # CHECK: 470784.0 478584.0 486384.0 494184.0 501984.0 509784.0 517584.0 525384.0
    # CHECK: 486144.0 494200.0 502256.0 510312.0 518368.0 526424.0 534480.0 542536.0
    # CHECK: 501632.0 509944.0 518256.0 526568.0 534880.0 543192.0 551504.0 559816.0
    # CHECK: 516992.0 525560.0 534128.0 542696.0 551264.0 559832.0 568400.0 576968.0
    # CHECK: 532352.0 541176.0 550000.0 558824.0 567648.0 576472.0 585296.0 594120.0
    # CHECK: 547712.0 556792.0 565872.0 574952.0 584032.0 593112.0 602192.0 611272.0
    # CHECK: 563072.0 572408.0 581744.0 591080.0 600416.0 609752.0 619088.0 628424.0
    # CHECK: 578432.0 588024.0 597616.0 607208.0 616800.0 626392.0 635984.0 645576.0
    # CHECK: 593792.0 603640.0 613488.0 623336.0 633184.0 643032.0 652880.0 662728.0
    # CHECK: 609152.0 619256.0 629360.0 639464.0 649568.0 659672.0 669776.0 679880.0
    # CHECK: 624512.0 634872.0 645232.0 655592.0 665952.0 676312.0 686672.0 697032.0
    # CHECK: 639872.0 650488.0 661104.0 671720.0 682336.0 692952.0 703568.0 714184.0
    # CHECK: 655232.0 666104.0 676976.0 687848.0 698720.0 709592.0 720464.0 731336.0
    # CHECK: 670592.0 681720.0 692848.0 703976.0 715104.0 726232.0 737360.0 748488.0
    # CHECK: 685952.0 697336.0 708720.0 720104.0 731488.0 742872.0 754256.0 765640.0
    # CHECK: 701312.0 712952.0 724592.0 736232.0 747872.0 759512.0 771152.0 782792.0
    # CHECK: 716672.0 728568.0 740464.0 752360.0 764256.0 776152.0 788048.0 799944.0
    # CHECK: 732032.0 744184.0 756336.0 768488.0 780640.0 792792.0 804944.0 817096.0
    # CHECK: 747392.0 759800.0 772208.0 784616.0 797024.0 809432.0 821840.0 834248.0
    # CHECK: 762752.0 775416.0 788080.0 800744.0 813408.0 826072.0 838736.0 851400.0
    # CHECK: 778112.0 791032.0 803952.0 816872.0 829792.0 842712.0 855632.0 868552.0
    # CHECK: 793472.0 806648.0 819824.0 833000.0 846176.0 859352.0 872528.0 885704.0
    # CHECK: 808832.0 822264.0 835696.0 849128.0 862560.0 875992.0 889424.0 902856.0
    # CHECK: 824192.0 837880.0 851568.0 865256.0 878944.0 892632.0 906320.0 920008.0
    # CHECK: 839552.0 853496.0 867440.0 881384.0 895328.0 909272.0 923216.0 937160.0
    # CHECK: 854912.0 869112.0 883312.0 897512.0 911712.0 925912.0 940112.0 954312.0
    # CHECK: 870272.0 884728.0 899184.0 913640.0 928096.0 942552.0 957008.0 971464.0
    # CHECK: 885632.0 900344.0 915056.0 929768.0 944480.0 959192.0 973904.0 988616.0
    # CHECK: 900992.0 915960.0 930928.0 945896.0 960864.0 975832.0 990800.0 1005768.0
    # CHECK: 916352.0 931576.0 946800.0 962024.0 977248.0 992472.0 1007696.0 1022920.0
    # CHECK: 931712.0 947192.0 962672.0 978152.0 993632.0 1009112.0 1024592.0 1040072.0
    # CHECK: 947072.0 962808.0 978544.0 994280.0 1010016.0 1025752.0 1041488.0 1057224.0
    # CHECK: 962432.0 978424.0 994416.0 1010408.0 1026400.0 1042392.0 1058384.0 1074376.0
    # CHECK: 977792.0 994040.0 1010288.0 1026536.0 1042784.0 1059032.0 1075280.0 1091528.0

    print(dst.tensor())

    _ = lhs^
    _ = rhs^
    _ = dst^
    _ = func^


def main():
    with DeviceContext() as ctx:
        test_tensor_core_async_tf32_tf32_64x8x8(ctx)
        test_tensor_core_async_bf16_bf16_f32_64x8x16(ctx)
